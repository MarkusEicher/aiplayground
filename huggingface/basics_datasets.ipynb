{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q datasets  python-dotenv torch transformers[torch] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a secret stored as an environment variable\n",
    "HF_TOKEN=os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tiny dataset subset\n",
    "dataset = load_dataset(\"squad\", split=\"train[:100]\")  # Only 100 examples\n",
    "eval_dataset = load_dataset(\"squad\", split=\"validation[:20]\")  # 20 validation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load smaller model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # Much smaller than BERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize questions and contexts\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],  # Now passing lists directly\n",
    "        examples[\"context\"],   # Now passing lists directly\n",
    "        truncation=\"only_second\",\n",
    "        max_length=256,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=False  # Disable overflow tokens for simplicity\n",
    "    )\n",
    "    \n",
    "    # Initialize answer arrays\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # Process each example in the batch\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        # Get the offset mapping for this example\n",
    "        offset = tokenized[\"offset_mapping\"][i]\n",
    "        \n",
    "        # Get answer for this example\n",
    "        answer = examples[\"answers\"][i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        \n",
    "        # Convert char positions to token positions\n",
    "        start_token = 0\n",
    "        end_token = 0\n",
    "        \n",
    "        # Find the token positions that contain the answer\n",
    "        for idx, (start, end) in enumerate(offset):\n",
    "            if start <= start_char and end >= start_char:\n",
    "                start_token = idx\n",
    "            if start <= end_char and end >= end_char:\n",
    "                end_token = idx\n",
    "                break\n",
    "        \n",
    "        start_positions.append(start_token)\n",
    "        end_positions.append(end_token)\n",
    "    \n",
    "    # Add answer positions to tokenized output\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Explicit batch size\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Explicit batch size\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./quick-qa-results\",\n",
    "    num_train_epochs=1,  # Single epoch\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,  # Slightly higher learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"no\",  # Skip evaluation to save time\n",
    "    save_strategy=\"no\",  # Don't save checkpoints\n",
    "    use_cpu=True,  # Force CPU\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./quick-qa-model\")\n",
    "tokenizer.save_pretrained(\"./quick-qa-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_model(model_path=\"./quick-qa-model\"):\n",
    "    # Load model and tokenizer from saved directory\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, model, tokenizer):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=\"only_second\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Find start and end positions\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    # Convert token positions to string\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    answer = tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end + 1])\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_qa_model()\n",
    "\n",
    "# Example context and question\n",
    "context = \"\"\"\n",
    "Python is a high-level programming language created by Guido van Rossum and released in 1991. \n",
    "Python's design emphasizes code readability with its notable use of significant whitespace. \n",
    "Its language constructs and object-oriented approach aim to help programmers write clear, logical code.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who created Python?\"\n",
    "\n",
    "# Get answer\n",
    "answer = answer_question(question, context, model, tokenizer)\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exampro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
